


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models and pre-trained weights &mdash; Torchvision main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom_torchvision.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="alexnet" href="generated/torchvision.models.alexnet.html" />
    <link rel="prev" title="vflip" href="generated/torchvision.transforms.functional.vflip.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/vision/versions.html'>main (0.12.0a0+9b5a3fe ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforming and augmenting images</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models and pre-trained weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="ops.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="io.html">Reading/Writing images and videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_extraction.html">Feature extraction for model inspection</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples and training references</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Example gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_references.html">Training references</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/docs">PyTorch</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Models and pre-trained weights</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/models.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="models-and-pre-trained-weights">
<span id="models"></span><h1>Models and pre-trained weights<a class="headerlink" href="#models-and-pre-trained-weights" title="Permalink to this headline">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> subpackage contains definitions of models for addressing
different tasks, including: image classification, pixelwise semantic
segmentation, object detection, instance segmentation, person
keypoint detection, video classification, and optical flow.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward compatibility is guaranteed for loading a serialized
<code class="docutils literal notranslate"><span class="pre">state_dict</span></code> to the model created using old PyTorch version.
On the contrary, loading entire saved models or serialized
<code class="docutils literal notranslate"><span class="pre">ScriptModules</span></code> (seralized using older versions of PyTorch)
may not preserve the historic behaviour. Refer to the following
<a class="reference external" href="https://pytorch.org/docs/stable/notes/serialization.html#id6">documentation</a></p>
</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>The models subpackage contains definitions for the following model
architectures for image classification:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1404.5997">AlexNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1409.1556">VGG</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1512.03385">ResNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1602.07360">SqueezeNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1608.06993">DenseNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1512.00567">Inception</a> v3</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1807.11164">ShuffleNet</a> v2</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1801.04381">MobileNetV2</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1905.02244">MobileNetV3</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1611.05431">ResNeXt</a></p></li>
<li><p><a class="reference internal" href="#wide-resnet">Wide ResNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1807.11626">MNASNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1905.11946">EfficientNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2003.13678">RegNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2010.11929">VisionTransformer</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2201.03545">ConvNeXt</a></p></li>
</ul>
<p>You can construct a model with random weights by calling its constructor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">alexnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">()</span>
<span class="n">vgg16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">()</span>
<span class="n">squeezenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">squeezenet1_0</span><span class="p">()</span>
<span class="n">densenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet161</span><span class="p">()</span>
<span class="n">inception</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">()</span>
<span class="n">googlenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">googlenet</span><span class="p">()</span>
<span class="n">shufflenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">shufflenet_v2_x1_0</span><span class="p">()</span>
<span class="n">mobilenet_v2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">()</span>
<span class="n">mobilenet_v3_large</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v3_large</span><span class="p">()</span>
<span class="n">mobilenet_v3_small</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v3_small</span><span class="p">()</span>
<span class="n">resnext50_32x4d</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnext50_32x4d</span><span class="p">()</span>
<span class="n">wide_resnet50_2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">wide_resnet50_2</span><span class="p">()</span>
<span class="n">mnasnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mnasnet1_0</span><span class="p">()</span>
<span class="n">efficientnet_b0</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b0</span><span class="p">()</span>
<span class="n">efficientnet_b1</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b1</span><span class="p">()</span>
<span class="n">efficientnet_b2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b2</span><span class="p">()</span>
<span class="n">efficientnet_b3</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b3</span><span class="p">()</span>
<span class="n">efficientnet_b4</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b4</span><span class="p">()</span>
<span class="n">efficientnet_b5</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b5</span><span class="p">()</span>
<span class="n">efficientnet_b6</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b6</span><span class="p">()</span>
<span class="n">efficientnet_b7</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b7</span><span class="p">()</span>
<span class="n">regnet_y_400mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_400mf</span><span class="p">()</span>
<span class="n">regnet_y_800mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_800mf</span><span class="p">()</span>
<span class="n">regnet_y_1_6gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_1_6gf</span><span class="p">()</span>
<span class="n">regnet_y_3_2gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_3_2gf</span><span class="p">()</span>
<span class="n">regnet_y_8gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_8gf</span><span class="p">()</span>
<span class="n">regnet_y_16gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_16gf</span><span class="p">()</span>
<span class="n">regnet_y_32gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_32gf</span><span class="p">()</span>
<span class="n">regnet_y_128gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_128gf</span><span class="p">()</span>
<span class="n">regnet_x_400mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_400mf</span><span class="p">()</span>
<span class="n">regnet_x_800mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_800mf</span><span class="p">()</span>
<span class="n">regnet_x_1_6gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_1_6gf</span><span class="p">()</span>
<span class="n">regnet_x_3_2gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_3_2gf</span><span class="p">()</span>
<span class="n">regnet_x_8gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_8gf</span><span class="p">()</span>
<span class="n">regnet_x_16gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_16gf</span><span class="p">()</span>
<span class="n">regnet_x_32gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_32gf</span><span class="p">()</span>
<span class="n">vit_b_16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">()</span>
<span class="n">vit_b_32</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_32</span><span class="p">()</span>
<span class="n">vit_l_16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_l_16</span><span class="p">()</span>
<span class="n">vit_l_32</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_l_32</span><span class="p">()</span>
<span class="n">convnext_tiny</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_tiny</span><span class="p">()</span>
<span class="n">convnext_small</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_small</span><span class="p">()</span>
<span class="n">convnext_base</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_base</span><span class="p">()</span>
<span class="n">convnext_large</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_large</span><span class="p">()</span>
</pre></div>
</div>
<p>We provide pre-trained models, using the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo" title="(in PyTorch v1.11.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.utils.model_zoo</span></code></a>.
These can be constructed by passing <code class="docutils literal notranslate"><span class="pre">pretrained=True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">alexnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">squeezenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">squeezenet1_0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vgg16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">densenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet161</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">inception</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">googlenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">googlenet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">shufflenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">shufflenet_v2_x1_0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mobilenet_v2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mobilenet_v3_large</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v3_large</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mobilenet_v3_small</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v3_small</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">resnext50_32x4d</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnext50_32x4d</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">wide_resnet50_2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">wide_resnet50_2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mnasnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">mnasnet1_0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b0</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b1</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b1</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b3</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b3</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b4</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b4</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b5</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b5</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b6</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b6</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">efficientnet_b7</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b7</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_400mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_400mf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_800mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_800mf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_1_6gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_1_6gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_3_2gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_3_2gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_8gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_8gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_16gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_16gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_y_32gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_y_32gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_400mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_400mf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_800mf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_800mf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_1_6gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_1_6gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_3_2gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_3_2gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_8gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_8gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">regnet_x_16gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_16gf</span><span class="p">(</span><span class="n">pretrainedTrue</span><span class="p">)</span>
<span class="n">regnet_x_32gf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">regnet_x_32gf</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vit_b_16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vit_b_32</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_32</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vit_l_16</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_l_16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vit_l_32</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_l_32</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">convnext_tiny</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_tiny</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">convnext_small</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_small</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">convnext_base</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_base</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">convnext_large</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">convnext_large</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Instancing a pre-trained model will download its weights to a cache directory.
This directory can be set using the <cite>TORCH_HOME</cite> environment variable. See
<a class="reference external" href="https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url" title="(in PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hub.load_state_dict_from_url()</span></code></a> for details.</p>
<p>Some models use modules which have different training and evaluation
behavior, such as batch normalization. To switch between these modes, use
<code class="docutils literal notranslate"><span class="pre">model.train()</span></code> or <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> as appropriate. See
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="(in PyTorch v1.11.0)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train()</span></code></a> or <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval" title="(in PyTorch v1.11.0)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eval()</span></code></a> for details.</p>
<p>All pre-trained models expect input images normalized in the same way,
i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),
where H and W are expected to be at least 224.
The images have to be loaded in to a range of [0, 1] and then normalized
using <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">[0.485,</span> <span class="pre">0.456,</span> <span class="pre">0.406]</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">[0.229,</span> <span class="pre">0.224,</span> <span class="pre">0.225]</span></code>.
You can use the following transform to normalize:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                 <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</pre></div>
</div>
<p>An example of such normalization can be found in the imagenet example
<a class="reference external" href="https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101">here</a></p>
<p>The process for obtaining the values of <cite>mean</cite> and <cite>std</cite> is roughly equivalent
to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span> <span class="k">as</span> <span class="n">T</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageNet</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">stds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
    <span class="n">stds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">means</span><span class="p">))</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">stds</span><span class="p">))</span>
</pre></div>
</div>
<p>Unfortunately, the concrete <cite>subset</cite> that was used is lost. For more
information see <a class="reference external" href="https://github.com/pytorch/vision/issues/1439">this discussion</a>
or <a class="reference external" href="https://github.com/pytorch/vision/pull/1965">these experiments</a>.</p>
<p>The sizes of the EfficientNet models depend on the variant. For the exact input sizes
<a class="reference external" href="https://github.com/pytorch/vision/blob/d2bfd639e46e1c5dc3c177f889dc7750c8d137c7/references/classification/train.py#L92-L93">check here</a></p>
<p>ImageNet 1-crop error rates</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Acc&#64;1</p></th>
<th class="head"><p>Acc&#64;5</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AlexNet</p></td>
<td><p>56.522</p></td>
<td><p>79.066</p></td>
</tr>
<tr class="row-odd"><td><p>VGG-11</p></td>
<td><p>69.020</p></td>
<td><p>88.628</p></td>
</tr>
<tr class="row-even"><td><p>VGG-13</p></td>
<td><p>69.928</p></td>
<td><p>89.246</p></td>
</tr>
<tr class="row-odd"><td><p>VGG-16</p></td>
<td><p>71.592</p></td>
<td><p>90.382</p></td>
</tr>
<tr class="row-even"><td><p>VGG-19</p></td>
<td><p>72.376</p></td>
<td><p>90.876</p></td>
</tr>
<tr class="row-odd"><td><p>VGG-11 with batch normalization</p></td>
<td><p>70.370</p></td>
<td><p>89.810</p></td>
</tr>
<tr class="row-even"><td><p>VGG-13 with batch normalization</p></td>
<td><p>71.586</p></td>
<td><p>90.374</p></td>
</tr>
<tr class="row-odd"><td><p>VGG-16 with batch normalization</p></td>
<td><p>73.360</p></td>
<td><p>91.516</p></td>
</tr>
<tr class="row-even"><td><p>VGG-19 with batch normalization</p></td>
<td><p>74.218</p></td>
<td><p>91.842</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet-18</p></td>
<td><p>69.758</p></td>
<td><p>89.078</p></td>
</tr>
<tr class="row-even"><td><p>ResNet-34</p></td>
<td><p>73.314</p></td>
<td><p>91.420</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet-50</p></td>
<td><p>76.130</p></td>
<td><p>92.862</p></td>
</tr>
<tr class="row-even"><td><p>ResNet-101</p></td>
<td><p>77.374</p></td>
<td><p>93.546</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet-152</p></td>
<td><p>78.312</p></td>
<td><p>94.046</p></td>
</tr>
<tr class="row-even"><td><p>SqueezeNet 1.0</p></td>
<td><p>58.092</p></td>
<td><p>80.420</p></td>
</tr>
<tr class="row-odd"><td><p>SqueezeNet 1.1</p></td>
<td><p>58.178</p></td>
<td><p>80.624</p></td>
</tr>
<tr class="row-even"><td><p>Densenet-121</p></td>
<td><p>74.434</p></td>
<td><p>91.972</p></td>
</tr>
<tr class="row-odd"><td><p>Densenet-169</p></td>
<td><p>75.600</p></td>
<td><p>92.806</p></td>
</tr>
<tr class="row-even"><td><p>Densenet-201</p></td>
<td><p>76.896</p></td>
<td><p>93.370</p></td>
</tr>
<tr class="row-odd"><td><p>Densenet-161</p></td>
<td><p>77.138</p></td>
<td><p>93.560</p></td>
</tr>
<tr class="row-even"><td><p>Inception v3</p></td>
<td><p>77.294</p></td>
<td><p>93.450</p></td>
</tr>
<tr class="row-odd"><td><p>GoogleNet</p></td>
<td><p>69.778</p></td>
<td><p>89.530</p></td>
</tr>
<tr class="row-even"><td><p>ShuffleNet V2 x1.0</p></td>
<td><p>69.362</p></td>
<td><p>88.316</p></td>
</tr>
<tr class="row-odd"><td><p>ShuffleNet V2 x0.5</p></td>
<td><p>60.552</p></td>
<td><p>81.746</p></td>
</tr>
<tr class="row-even"><td><p>MobileNet V2</p></td>
<td><p>71.878</p></td>
<td><p>90.286</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet V3 Large</p></td>
<td><p>74.042</p></td>
<td><p>91.340</p></td>
</tr>
<tr class="row-even"><td><p>MobileNet V3 Small</p></td>
<td><p>67.668</p></td>
<td><p>87.402</p></td>
</tr>
<tr class="row-odd"><td><p>ResNeXt-50-32x4d</p></td>
<td><p>77.618</p></td>
<td><p>93.698</p></td>
</tr>
<tr class="row-even"><td><p>ResNeXt-101-32x8d</p></td>
<td><p>79.312</p></td>
<td><p>94.526</p></td>
</tr>
<tr class="row-odd"><td><p>Wide ResNet-50-2</p></td>
<td><p>78.468</p></td>
<td><p>94.086</p></td>
</tr>
<tr class="row-even"><td><p>Wide ResNet-101-2</p></td>
<td><p>78.848</p></td>
<td><p>94.284</p></td>
</tr>
<tr class="row-odd"><td><p>MNASNet 1.0</p></td>
<td><p>73.456</p></td>
<td><p>91.510</p></td>
</tr>
<tr class="row-even"><td><p>MNASNet 0.5</p></td>
<td><p>67.734</p></td>
<td><p>87.490</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-B0</p></td>
<td><p>77.692</p></td>
<td><p>93.532</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B1</p></td>
<td><p>78.642</p></td>
<td><p>94.186</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-B2</p></td>
<td><p>80.608</p></td>
<td><p>95.310</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B3</p></td>
<td><p>82.008</p></td>
<td><p>96.054</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-B4</p></td>
<td><p>83.384</p></td>
<td><p>96.594</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B5</p></td>
<td><p>83.444</p></td>
<td><p>96.628</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-B6</p></td>
<td><p>84.008</p></td>
<td><p>96.916</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B7</p></td>
<td><p>84.122</p></td>
<td><p>96.908</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_x_400mf</p></td>
<td><p>72.834</p></td>
<td><p>90.950</p></td>
</tr>
<tr class="row-even"><td><p>regnet_x_800mf</p></td>
<td><p>75.212</p></td>
<td><p>92.348</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_x_1_6gf</p></td>
<td><p>77.040</p></td>
<td><p>93.440</p></td>
</tr>
<tr class="row-even"><td><p>regnet_x_3_2gf</p></td>
<td><p>78.364</p></td>
<td><p>93.992</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_x_8gf</p></td>
<td><p>79.344</p></td>
<td><p>94.686</p></td>
</tr>
<tr class="row-even"><td><p>regnet_x_16gf</p></td>
<td><p>80.058</p></td>
<td><p>94.944</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_x_32gf</p></td>
<td><p>80.622</p></td>
<td><p>95.248</p></td>
</tr>
<tr class="row-even"><td><p>regnet_y_400mf</p></td>
<td><p>74.046</p></td>
<td><p>91.716</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_y_800mf</p></td>
<td><p>76.420</p></td>
<td><p>93.136</p></td>
</tr>
<tr class="row-even"><td><p>regnet_y_1_6gf</p></td>
<td><p>77.950</p></td>
<td><p>93.966</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_y_3_2gf</p></td>
<td><p>78.948</p></td>
<td><p>94.576</p></td>
</tr>
<tr class="row-even"><td><p>regnet_y_8gf</p></td>
<td><p>80.032</p></td>
<td><p>95.048</p></td>
</tr>
<tr class="row-odd"><td><p>regnet_y_16gf</p></td>
<td><p>80.424</p></td>
<td><p>95.240</p></td>
</tr>
<tr class="row-even"><td><p>regnet_y_32gf</p></td>
<td><p>80.878</p></td>
<td><p>95.340</p></td>
</tr>
<tr class="row-odd"><td><p>vit_b_16</p></td>
<td><p>81.072</p></td>
<td><p>95.318</p></td>
</tr>
<tr class="row-even"><td><p>vit_b_32</p></td>
<td><p>75.912</p></td>
<td><p>92.466</p></td>
</tr>
<tr class="row-odd"><td><p>vit_l_16</p></td>
<td><p>79.662</p></td>
<td><p>94.638</p></td>
</tr>
<tr class="row-even"><td><p>vit_l_32</p></td>
<td><p>76.972</p></td>
<td><p>93.070</p></td>
</tr>
<tr class="row-odd"><td><p>convnext_tiny</p></td>
<td><p>82.520</p></td>
<td><p>96.146</p></td>
</tr>
<tr class="row-even"><td><p>convnext_small</p></td>
<td><p>83.616</p></td>
<td><p>96.650</p></td>
</tr>
<tr class="row-odd"><td><p>convnext_base</p></td>
<td><p>84.062</p></td>
<td><p>96.870</p></td>
</tr>
<tr class="row-even"><td><p>convnext_large</p></td>
<td><p>84.414</p></td>
<td><p>96.976</p></td>
</tr>
</tbody>
</table>
<div class="section" id="id1">
<h3>Alexnet<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.alexnet.html#torchvision.models.alexnet" title="torchvision.models.alexnet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">alexnet</span></code></a>([pretrained, progress])</p></td>
<td><p>AlexNet model architecture from the <a class="reference external" href="https://arxiv.org/abs/1404.5997">“One weird trick…”</a> paper.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id2">
<h3>VGG<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vgg11.html#torchvision.models.vgg11" title="torchvision.models.vgg11"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg11</span></code></a>([pretrained, progress])</p></td>
<td><p>VGG 11-layer model (configuration “A”) from <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vgg11_bn.html#torchvision.models.vgg11_bn" title="torchvision.models.vgg11_bn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg11_bn</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 11-layer model (configuration “A”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vgg13.html#torchvision.models.vgg13" title="torchvision.models.vgg13"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg13</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 13-layer model (configuration “B”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vgg13_bn.html#torchvision.models.vgg13_bn" title="torchvision.models.vgg13_bn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg13_bn</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 13-layer model (configuration “B”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vgg16.html#torchvision.models.vgg16" title="torchvision.models.vgg16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg16</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 16-layer model (configuration “D”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vgg16_bn.html#torchvision.models.vgg16_bn" title="torchvision.models.vgg16_bn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg16_bn</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 16-layer model (configuration “D”) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vgg19.html#torchvision.models.vgg19" title="torchvision.models.vgg19"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg19</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 19-layer model (configuration “E”) <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vgg19_bn.html#torchvision.models.vgg19_bn" title="torchvision.models.vgg19_bn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vgg19_bn</span></code></a>([pretrained, progress])</p></td>
<td><p><p>VGG 19-layer model (configuration ‘E’) with batch normalization <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">“Very Deep Convolutional Networks For Large-Scale Image Recognition”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id10">
<h3>ResNet<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.resnet18.html#torchvision.models.resnet18" title="torchvision.models.resnet18"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnet18</span></code></a>([pretrained, progress])</p></td>
<td><p>ResNet-18 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.resnet34.html#torchvision.models.resnet34" title="torchvision.models.resnet34"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnet34</span></code></a>([pretrained, progress])</p></td>
<td><p><p>ResNet-34 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.resnet50.html#torchvision.models.resnet50" title="torchvision.models.resnet50"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnet50</span></code></a>([pretrained, progress])</p></td>
<td><p><p>ResNet-50 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.resnet101.html#torchvision.models.resnet101" title="torchvision.models.resnet101"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnet101</span></code></a>([pretrained, progress])</p></td>
<td><p><p>ResNet-101 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.resnet152.html#torchvision.models.resnet152" title="torchvision.models.resnet152"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnet152</span></code></a>([pretrained, progress])</p></td>
<td><p><p>ResNet-152 model from <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">“Deep Residual Learning for Image Recognition”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id15">
<h3>SqueezeNet<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.squeezenet1_0.html#torchvision.models.squeezenet1_0" title="torchvision.models.squeezenet1_0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeezenet1_0</span></code></a>([pretrained, progress])</p></td>
<td><p>SqueezeNet model architecture from the <a class="reference external" href="https://arxiv.org/abs/1602.07360">“SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size”</a> paper.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.squeezenet1_1.html#torchvision.models.squeezenet1_1" title="torchvision.models.squeezenet1_1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeezenet1_1</span></code></a>([pretrained, progress])</p></td>
<td><p>SqueezeNet 1.1 model from the <a class="reference external" href="https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1">official SqueezeNet repo</a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id16">
<h3>DenseNet<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.densenet121.html#torchvision.models.densenet121" title="torchvision.models.densenet121"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densenet121</span></code></a>([pretrained, progress])</p></td>
<td><p>Densenet-121 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.densenet169.html#torchvision.models.densenet169" title="torchvision.models.densenet169"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densenet169</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Densenet-169 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.densenet161.html#torchvision.models.densenet161" title="torchvision.models.densenet161"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densenet161</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Densenet-161 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.densenet201.html#torchvision.models.densenet201" title="torchvision.models.densenet201"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densenet201</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Densenet-201 model from <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">“Densely Connected Convolutional Networks”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="inception-v3">
<h3>Inception v3<a class="headerlink" href="#inception-v3" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.inception_v3.html#torchvision.models.inception_v3" title="torchvision.models.inception_v3"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inception_v3</span></code></a>([pretrained, progress])</p></td>
<td><p>Inception v3 model architecture from <a class="reference external" href="http://arxiv.org/abs/1512.00567">“Rethinking the Inception Architecture for Computer Vision”</a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id20">
<h3>GoogLeNet<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.googlenet.html#torchvision.models.googlenet" title="torchvision.models.googlenet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">googlenet</span></code></a>([pretrained, progress])</p></td>
<td><p>GoogLeNet (Inception v1) model architecture from <a class="reference external" href="http://arxiv.org/abs/1409.4842">“Going Deeper with Convolutions”</a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="shufflenet-v2">
<h3>ShuffleNet v2<a class="headerlink" href="#shufflenet-v2" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.shufflenet_v2_x0_5" title="torchvision.models.shufflenet_v2_x0_5"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shufflenet_v2_x0_5</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a ShuffleNetV2 with 0.5x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.shufflenet_v2_x1_0.html#torchvision.models.shufflenet_v2_x1_0" title="torchvision.models.shufflenet_v2_x1_0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shufflenet_v2_x1_0</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a ShuffleNetV2 with 1.0x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.shufflenet_v2_x1_5.html#torchvision.models.shufflenet_v2_x1_5" title="torchvision.models.shufflenet_v2_x1_5"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shufflenet_v2_x1_5</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a ShuffleNetV2 with 1.5x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.shufflenet_v2_x2_0.html#torchvision.models.shufflenet_v2_x2_0" title="torchvision.models.shufflenet_v2_x2_0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shufflenet_v2_x2_0</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a ShuffleNetV2 with 2.0x output channels, as described in <a class="reference external" href="https://arxiv.org/abs/1807.11164">“ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="mobilenet-v2">
<h3>MobileNet v2<a class="headerlink" href="#mobilenet-v2" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.mobilenet_v2.html#torchvision.models.mobilenet_v2" title="torchvision.models.mobilenet_v2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mobilenet_v2</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a MobileNetV2 architecture from <a class="reference external" href="https://arxiv.org/abs/1801.04381">“MobileNetV2: Inverted Residuals and Linear Bottlenecks”</a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="mobilenet-v3">
<h3>MobileNet v3<a class="headerlink" href="#mobilenet-v3" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.mobilenet_v3_large" title="torchvision.models.mobilenet_v3_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mobilenet_v3_large</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a large MobileNetV3 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.02244">“Searching for MobileNetV3”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.mobilenet_v3_small" title="torchvision.models.mobilenet_v3_small"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mobilenet_v3_small</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a small MobileNetV3 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.02244">“Searching for MobileNetV3”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id25">
<h3>ResNext<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.resnext50_32x4d.html#torchvision.models.resnext50_32x4d" title="torchvision.models.resnext50_32x4d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnext50_32x4d</span></code></a>([pretrained, progress])</p></td>
<td><p>ResNeXt-50 32x4d model from <a class="reference external" href="https://arxiv.org/pdf/1611.05431.pdf">“Aggregated Residual Transformation for Deep Neural Networks”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.resnext101_32x8d.html#torchvision.models.resnext101_32x8d" title="torchvision.models.resnext101_32x8d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resnext101_32x8d</span></code></a>([pretrained, progress])</p></td>
<td><p><p>ResNeXt-101 32x8d model from <a class="reference external" href="https://arxiv.org/pdf/1611.05431.pdf">“Aggregated Residual Transformation for Deep Neural Networks”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="wide-resnet">
<h3>Wide ResNet<a class="headerlink" href="#wide-resnet" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.wide_resnet50_2.html#torchvision.models.wide_resnet50_2" title="torchvision.models.wide_resnet50_2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wide_resnet50_2</span></code></a>([pretrained, progress])</p></td>
<td><p>Wide ResNet-50-2 model from <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">“Wide Residual Networks”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.wide_resnet101_2.html#torchvision.models.wide_resnet101_2" title="torchvision.models.wide_resnet101_2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wide_resnet101_2</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Wide ResNet-101-2 model from <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">“Wide Residual Networks”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id28">
<h3>MNASNet<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.mnasnet0_5.html#torchvision.models.mnasnet0_5" title="torchvision.models.mnasnet0_5"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mnasnet0_5</span></code></a>([pretrained, progress])</p></td>
<td><p>MNASNet with depth multiplier of 0.5 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.mnasnet0_75.html#torchvision.models.mnasnet0_75" title="torchvision.models.mnasnet0_75"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mnasnet0_75</span></code></a>([pretrained, progress])</p></td>
<td><p><p>MNASNet with depth multiplier of 0.75 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.mnasnet1_0.html#torchvision.models.mnasnet1_0" title="torchvision.models.mnasnet1_0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mnasnet1_0</span></code></a>([pretrained, progress])</p></td>
<td><p><p>MNASNet with depth multiplier of 1.0 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.mnasnet1_3.html#torchvision.models.mnasnet1_3" title="torchvision.models.mnasnet1_3"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mnasnet1_3</span></code></a>([pretrained, progress])</p></td>
<td><p><p>MNASNet with depth multiplier of 1.3 from <a class="reference external" href="https://arxiv.org/pdf/1807.11626.pdf">“MnasNet: Platform-Aware Neural Architecture Search for Mobile”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id32">
<h3>EfficientNet<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b0.html#torchvision.models.efficientnet_b0" title="torchvision.models.efficientnet_b0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b0</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a EfficientNet B0 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b1.html#torchvision.models.efficientnet_b1" title="torchvision.models.efficientnet_b1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b1</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B1 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2" title="torchvision.models.efficientnet_b2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b2</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B2 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b3.html#torchvision.models.efficientnet_b3" title="torchvision.models.efficientnet_b3"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b3</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B3 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b4.html#torchvision.models.efficientnet_b4" title="torchvision.models.efficientnet_b4"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b4</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B4 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b5.html#torchvision.models.efficientnet_b5" title="torchvision.models.efficientnet_b5"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b5</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B5 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b6.html#torchvision.models.efficientnet_b6" title="torchvision.models.efficientnet_b6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b6</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B6 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.efficientnet_b7.html#torchvision.models.efficientnet_b7" title="torchvision.models.efficientnet_b7"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_b7</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a EfficientNet B7 architecture from <a class="reference external" href="https://arxiv.org/abs/1905.11946">“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id40">
<h3>RegNet<a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_400mf.html#torchvision.models.regnet_y_400mf" title="torchvision.models.regnet_y_400mf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_400mf</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a RegNetY_400MF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_800mf.html#torchvision.models.regnet_y_800mf" title="torchvision.models.regnet_y_800mf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_800mf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_800MF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.regnet_y_1_6gf" title="torchvision.models.regnet_y_1_6gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_1_6gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_1.6GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.regnet_y_3_2gf" title="torchvision.models.regnet_y_3_2gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_3_2gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_3.2GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_8gf.html#torchvision.models.regnet_y_8gf" title="torchvision.models.regnet_y_8gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_8gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_8GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_16gf.html#torchvision.models.regnet_y_16gf" title="torchvision.models.regnet_y_16gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_16gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_16GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_32gf.html#torchvision.models.regnet_y_32gf" title="torchvision.models.regnet_y_32gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_32gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_32GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_y_128gf.html#torchvision.models.regnet_y_128gf" title="torchvision.models.regnet_y_128gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_y_128gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetY_128GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_400mf.html#torchvision.models.regnet_x_400mf" title="torchvision.models.regnet_x_400mf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_400mf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_400MF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_800mf.html#torchvision.models.regnet_x_800mf" title="torchvision.models.regnet_x_800mf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_800mf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_800MF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.regnet_x_1_6gf" title="torchvision.models.regnet_x_1_6gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_1_6gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_1.6GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.regnet_x_3_2gf" title="torchvision.models.regnet_x_3_2gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_3_2gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_3.2GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_8gf.html#torchvision.models.regnet_x_8gf" title="torchvision.models.regnet_x_8gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_8gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_8GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_16gf.html#torchvision.models.regnet_x_16gf" title="torchvision.models.regnet_x_16gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_16gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_16GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.regnet_x_32gf.html#torchvision.models.regnet_x_32gf" title="torchvision.models.regnet_x_32gf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regnet_x_32gf</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a RegNetX_32GF architecture from <a class="reference external" href="https://arxiv.org/abs/2003.13678">“Designing Network Design Spaces”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id55">
<h3>VisionTransformer<a class="headerlink" href="#id55" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16" title="torchvision.models.vit_b_16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vit_b_16</span></code></a>([pretrained, progress])</p></td>
<td><p>Constructs a vit_b_16 architecture from <a class="reference external" href="https://arxiv.org/abs/2010.11929">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vit_b_32.html#torchvision.models.vit_b_32" title="torchvision.models.vit_b_32"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vit_b_32</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a vit_b_32 architecture from <a class="reference external" href="https://arxiv.org/abs/2010.11929">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</a>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.vit_l_16.html#torchvision.models.vit_l_16" title="torchvision.models.vit_l_16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vit_l_16</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a vit_l_16 architecture from <a class="reference external" href="https://arxiv.org/abs/2010.11929">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</a>.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.vit_l_32.html#torchvision.models.vit_l_32" title="torchvision.models.vit_l_32"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vit_l_32</span></code></a>([pretrained, progress])</p></td>
<td><p><p>Constructs a vit_l_32 architecture from <a class="reference external" href="https://arxiv.org/abs/2010.11929">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id59">
<h3>ConvNeXt<a class="headerlink" href="#id59" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.convnext_tiny.html#torchvision.models.convnext_tiny" title="torchvision.models.convnext_tiny"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convnext_tiny</span></code></a>(*[, pretrained, progress])</p></td>
<td><p>ConvNeXt Tiny model architecture from the <a class="reference external" href="https://arxiv.org/abs/2201.03545">“A ConvNet for the 2020s”</a> paper.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.convnext_small.html#torchvision.models.convnext_small" title="torchvision.models.convnext_small"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convnext_small</span></code></a>(*[, pretrained, progress])</p></td>
<td><p><p>ConvNeXt Small model architecture from the <a class="reference external" href="https://arxiv.org/abs/2201.03545">“A ConvNet for the 2020s”</a> paper.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.convnext_base.html#torchvision.models.convnext_base" title="torchvision.models.convnext_base"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convnext_base</span></code></a>(*[, pretrained, progress])</p></td>
<td><p><p>ConvNeXt Base model architecture from the <a class="reference external" href="https://arxiv.org/abs/2201.03545">“A ConvNet for the 2020s”</a> paper.</p>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.convnext_large.html#torchvision.models.convnext_large" title="torchvision.models.convnext_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convnext_large</span></code></a>(*[, pretrained, progress])</p></td>
<td><p><p>ConvNeXt Large model architecture from the <a class="reference external" href="https://arxiv.org/abs/2201.03545">“A ConvNet for the 2020s”</a> paper.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="quantized-models">
<h3>Quantized Models<a class="headerlink" href="#quantized-models" title="Permalink to this headline">¶</a></h3>
<p>The following architectures provide support for INT8 quantized models. You can get
a model with random weights by calling its constructor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="n">googlenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">googlenet</span><span class="p">()</span>
<span class="n">inception_v3</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">()</span>
<span class="n">mobilenet_v2</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">()</span>
<span class="n">mobilenet_v3_large</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">mobilenet_v3_large</span><span class="p">()</span>
<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">resnet50</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">resnet50</span><span class="p">()</span>
<span class="n">resnext101_32x8d</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">resnext101_32x8d</span><span class="p">()</span>
<span class="n">shufflenet_v2_x0_5</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">shufflenet_v2_x0_5</span><span class="p">()</span>
<span class="n">shufflenet_v2_x1_0</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">shufflenet_v2_x1_0</span><span class="p">()</span>
<span class="n">shufflenet_v2_x1_5</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">shufflenet_v2_x1_5</span><span class="p">()</span>
<span class="n">shufflenet_v2_x2_0</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">shufflenet_v2_x2_0</span><span class="p">()</span>
</pre></div>
</div>
<p>Obtaining a pre-trained quantized model can be done with a few lines of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># run the model with quantized inputs and weights</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</pre></div>
</div>
<p>We provide pre-trained quantized weights for the following models:</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Acc&#64;1</p></th>
<th class="head"><p>Acc&#64;5</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MobileNet V2</p></td>
<td><p>71.658</p></td>
<td><p>90.150</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet V3 Large</p></td>
<td><p>73.004</p></td>
<td><p>90.858</p></td>
</tr>
<tr class="row-even"><td><p>ShuffleNet V2 x1.0</p></td>
<td><p>68.360</p></td>
<td><p>87.582</p></td>
</tr>
<tr class="row-odd"><td><p>ShuffleNet V2 x0.5</p></td>
<td><p>57.972</p></td>
<td><p>79.780</p></td>
</tr>
<tr class="row-even"><td><p>ResNet 18</p></td>
<td><p>69.494</p></td>
<td><p>88.882</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet 50</p></td>
<td><p>75.920</p></td>
<td><p>92.814</p></td>
</tr>
<tr class="row-even"><td><p>ResNext 101 32x8d</p></td>
<td><p>78.986</p></td>
<td><p>94.480</p></td>
</tr>
<tr class="row-odd"><td><p>Inception V3</p></td>
<td><p>77.176</p></td>
<td><p>93.354</p></td>
</tr>
<tr class="row-even"><td><p>GoogleNet</p></td>
<td><p>69.826</p></td>
<td><p>89.404</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="semantic-segmentation">
<h2>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h2>
<p>The models subpackage contains definitions for the following model
architectures for semantic segmentation:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1411.4038">FCN ResNet50, ResNet101</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1706.05587">DeepLabV3 ResNet50, ResNet101, MobileNetV3-Large</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1905.02244">LR-ASPP MobileNetV3-Large</a></p></li>
</ul>
<p>As with image classification models, all pre-trained models expect input images normalized in the same way.
The images have to be loaded in to a range of <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code> and then normalized using
<code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">[0.485,</span> <span class="pre">0.456,</span> <span class="pre">0.406]</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">[0.229,</span> <span class="pre">0.224,</span> <span class="pre">0.225]</span></code>.
They have been trained on images resized such that their minimum size is 520.</p>
<p>For details on how to plot the masks of such models, you may refer to <a class="reference internal" href="auto_examples/plot_visualization_utils.html#semantic-seg-output"><span class="std std-ref">Semantic segmentation models</span></a>.</p>
<p>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are
present in the Pascal VOC dataset. You can see more information on how the subset has been selected in
<code class="docutils literal notranslate"><span class="pre">references/segmentation/coco_utils.py</span></code>. The classes that the pre-trained model outputs are the following,
in order:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;__background__&#39;</span><span class="p">,</span> <span class="s1">&#39;aeroplane&#39;</span><span class="p">,</span> <span class="s1">&#39;bicycle&#39;</span><span class="p">,</span> <span class="s1">&#39;bird&#39;</span><span class="p">,</span> <span class="s1">&#39;boat&#39;</span><span class="p">,</span> <span class="s1">&#39;bottle&#39;</span><span class="p">,</span> <span class="s1">&#39;bus&#39;</span><span class="p">,</span>
 <span class="s1">&#39;car&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;chair&#39;</span><span class="p">,</span> <span class="s1">&#39;cow&#39;</span><span class="p">,</span> <span class="s1">&#39;diningtable&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span> <span class="s1">&#39;motorbike&#39;</span><span class="p">,</span>
 <span class="s1">&#39;person&#39;</span><span class="p">,</span> <span class="s1">&#39;pottedplant&#39;</span><span class="p">,</span> <span class="s1">&#39;sheep&#39;</span><span class="p">,</span> <span class="s1">&#39;sofa&#39;</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;tvmonitor&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>mean IoU</p></th>
<th class="head"><p>global pixelwise acc</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FCN ResNet50</p></td>
<td><p>60.5</p></td>
<td><p>91.4</p></td>
</tr>
<tr class="row-odd"><td><p>FCN ResNet101</p></td>
<td><p>63.7</p></td>
<td><p>91.9</p></td>
</tr>
<tr class="row-even"><td><p>DeepLabV3 ResNet50</p></td>
<td><p>66.4</p></td>
<td><p>92.4</p></td>
</tr>
<tr class="row-odd"><td><p>DeepLabV3 ResNet101</p></td>
<td><p>67.4</p></td>
<td><p>92.4</p></td>
</tr>
<tr class="row-even"><td><p>DeepLabV3 MobileNetV3-Large</p></td>
<td><p>60.3</p></td>
<td><p>91.2</p></td>
</tr>
<tr class="row-odd"><td><p>LR-ASPP MobileNetV3-Large</p></td>
<td><p>57.9</p></td>
<td><p>91.2</p></td>
</tr>
</tbody>
</table>
<div class="section" id="fully-convolutional-networks">
<h3>Fully Convolutional Networks<a class="headerlink" href="#fully-convolutional-networks" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.fcn_resnet50.html#torchvision.models.segmentation.fcn_resnet50" title="torchvision.models.segmentation.fcn_resnet50"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.fcn_resnet50</span></code></a>([…])</p></td>
<td><p>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.fcn_resnet101.html#torchvision.models.segmentation.fcn_resnet101" title="torchvision.models.segmentation.fcn_resnet101"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.fcn_resnet101</span></code></a>([…])</p></td>
<td><p>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="deeplabv3">
<h3>DeepLabV3<a class="headerlink" href="#deeplabv3" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.deeplabv3_resnet50" title="torchvision.models.segmentation.deeplabv3_resnet50"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.deeplabv3_resnet50</span></code></a>([…])</p></td>
<td><p>Constructs a DeepLabV3 model with a ResNet-50 backbone.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.deeplabv3_resnet101" title="torchvision.models.segmentation.deeplabv3_resnet101"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.deeplabv3_resnet101</span></code></a>([…])</p></td>
<td><p>Constructs a DeepLabV3 model with a ResNet-101 backbone.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.deeplabv3_mobilenet_v3_large.html#torchvision.models.segmentation.deeplabv3_mobilenet_v3_large" title="torchvision.models.segmentation.deeplabv3_mobilenet_v3_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.deeplabv3_mobilenet_v3_large</span></code></a>([…])</p></td>
<td><p>Constructs a DeepLabV3 model with a MobileNetV3-Large backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="lr-aspp">
<h3>LR-ASPP<a class="headerlink" href="#lr-aspp" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html#torchvision.models.segmentation.lraspp_mobilenet_v3_large" title="torchvision.models.segmentation.lraspp_mobilenet_v3_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.segmentation.lraspp_mobilenet_v3_large</span></code></a>([…])</p></td>
<td><p>Constructs a Lite R-ASPP Network model with a MobileNetV3-Large backbone.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="object-detection-instance-segmentation-and-person-keypoint-detection">
<span id="object-det-inst-seg-pers-keypoint-det"></span><h2>Object Detection, Instance Segmentation and Person Keypoint Detection<a class="headerlink" href="#object-detection-instance-segmentation-and-person-keypoint-detection" title="Permalink to this headline">¶</a></h2>
<p>The models subpackage contains definitions for the following model
architectures for detection:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1904.01355">FCOS</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1708.02002">RetinaNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1512.02325">SSD</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1801.04381">SSDlite</a></p></li>
</ul>
<p>The pre-trained models for detection, instance segmentation and
keypoint detection are initialized with the classification models
in torchvision.</p>
<p>The models expect a list of <code class="docutils literal notranslate"><span class="pre">Tensor[C,</span> <span class="pre">H,</span> <span class="pre">W]</span></code>, in the range <code class="docutils literal notranslate"><span class="pre">0-1</span></code>.
The models internally resize the images but the behaviour varies depending
on the model. Check the constructor of the models for more information. The
output format of such models is illustrated in <a class="reference internal" href="auto_examples/plot_visualization_utils.html#instance-seg-output"><span class="std std-ref">Instance segmentation models</span></a>.</p>
<p>For object detection and instance segmentation, the pre-trained
models return the predictions of the following classes:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">COCO_INSTANCE_CATEGORY_NAMES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;__background__&#39;</span><span class="p">,</span> <span class="s1">&#39;person&#39;</span><span class="p">,</span> <span class="s1">&#39;bicycle&#39;</span><span class="p">,</span> <span class="s1">&#39;car&#39;</span><span class="p">,</span> <span class="s1">&#39;motorcycle&#39;</span><span class="p">,</span> <span class="s1">&#39;airplane&#39;</span><span class="p">,</span> <span class="s1">&#39;bus&#39;</span><span class="p">,</span>
    <span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;truck&#39;</span><span class="p">,</span> <span class="s1">&#39;boat&#39;</span><span class="p">,</span> <span class="s1">&#39;traffic light&#39;</span><span class="p">,</span> <span class="s1">&#39;fire hydrant&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;stop sign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;parking meter&#39;</span><span class="p">,</span> <span class="s1">&#39;bench&#39;</span><span class="p">,</span> <span class="s1">&#39;bird&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span> <span class="s1">&#39;sheep&#39;</span><span class="p">,</span> <span class="s1">&#39;cow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;elephant&#39;</span><span class="p">,</span> <span class="s1">&#39;bear&#39;</span><span class="p">,</span> <span class="s1">&#39;zebra&#39;</span><span class="p">,</span> <span class="s1">&#39;giraffe&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;backpack&#39;</span><span class="p">,</span> <span class="s1">&#39;umbrella&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span>
    <span class="s1">&#39;handbag&#39;</span><span class="p">,</span> <span class="s1">&#39;tie&#39;</span><span class="p">,</span> <span class="s1">&#39;suitcase&#39;</span><span class="p">,</span> <span class="s1">&#39;frisbee&#39;</span><span class="p">,</span> <span class="s1">&#39;skis&#39;</span><span class="p">,</span> <span class="s1">&#39;snowboard&#39;</span><span class="p">,</span> <span class="s1">&#39;sports ball&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kite&#39;</span><span class="p">,</span> <span class="s1">&#39;baseball bat&#39;</span><span class="p">,</span> <span class="s1">&#39;baseball glove&#39;</span><span class="p">,</span> <span class="s1">&#39;skateboard&#39;</span><span class="p">,</span> <span class="s1">&#39;surfboard&#39;</span><span class="p">,</span> <span class="s1">&#39;tennis racket&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bottle&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;wine glass&#39;</span><span class="p">,</span> <span class="s1">&#39;cup&#39;</span><span class="p">,</span> <span class="s1">&#39;fork&#39;</span><span class="p">,</span> <span class="s1">&#39;knife&#39;</span><span class="p">,</span> <span class="s1">&#39;spoon&#39;</span><span class="p">,</span> <span class="s1">&#39;bowl&#39;</span><span class="p">,</span>
    <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;sandwich&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;broccoli&#39;</span><span class="p">,</span> <span class="s1">&#39;carrot&#39;</span><span class="p">,</span> <span class="s1">&#39;hot dog&#39;</span><span class="p">,</span> <span class="s1">&#39;pizza&#39;</span><span class="p">,</span>
    <span class="s1">&#39;donut&#39;</span><span class="p">,</span> <span class="s1">&#39;cake&#39;</span><span class="p">,</span> <span class="s1">&#39;chair&#39;</span><span class="p">,</span> <span class="s1">&#39;couch&#39;</span><span class="p">,</span> <span class="s1">&#39;potted plant&#39;</span><span class="p">,</span> <span class="s1">&#39;bed&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;dining table&#39;</span><span class="p">,</span>
    <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;toilet&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;tv&#39;</span><span class="p">,</span> <span class="s1">&#39;laptop&#39;</span><span class="p">,</span> <span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;remote&#39;</span><span class="p">,</span> <span class="s1">&#39;keyboard&#39;</span><span class="p">,</span> <span class="s1">&#39;cell phone&#39;</span><span class="p">,</span>
    <span class="s1">&#39;microwave&#39;</span><span class="p">,</span> <span class="s1">&#39;oven&#39;</span><span class="p">,</span> <span class="s1">&#39;toaster&#39;</span><span class="p">,</span> <span class="s1">&#39;sink&#39;</span><span class="p">,</span> <span class="s1">&#39;refrigerator&#39;</span><span class="p">,</span> <span class="s1">&#39;N/A&#39;</span><span class="p">,</span> <span class="s1">&#39;book&#39;</span><span class="p">,</span>
    <span class="s1">&#39;clock&#39;</span><span class="p">,</span> <span class="s1">&#39;vase&#39;</span><span class="p">,</span> <span class="s1">&#39;scissors&#39;</span><span class="p">,</span> <span class="s1">&#39;teddy bear&#39;</span><span class="p">,</span> <span class="s1">&#39;hair drier&#39;</span><span class="p">,</span> <span class="s1">&#39;toothbrush&#39;</span>
<span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>Here are the summary of the accuracies for the models trained on
the instances set of COCO train2017 and evaluated on COCO val2017.</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>box AP</p></th>
<th class="head"><p>mask AP</p></th>
<th class="head"><p>keypoint AP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Faster R-CNN ResNet-50 FPN</p></td>
<td><p>37.0</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Faster R-CNN MobileNetV3-Large FPN</p></td>
<td><p>32.8</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>Faster R-CNN MobileNetV3-Large 320 FPN</p></td>
<td><p>22.8</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>FCOS ResNet-50 FPN</p></td>
<td><p>39.2</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>RetinaNet ResNet-50 FPN</p></td>
<td><p>36.4</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>SSD300 VGG16</p></td>
<td><p>25.1</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>SSDlite320 MobileNetV3-Large</p></td>
<td><p>21.3</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Mask R-CNN ResNet-50 FPN</p></td>
<td><p>37.9</p></td>
<td><p>34.6</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For person keypoint detection, the accuracies for the pre-trained
models are as follows</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>box AP</p></th>
<th class="head"><p>mask AP</p></th>
<th class="head"><p>keypoint AP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Keypoint R-CNN ResNet-50 FPN</p></td>
<td><p>54.6</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>65.0</p></td>
</tr>
</tbody>
</table>
<p>For person keypoint detection, the pre-trained model return the
keypoints in the following order:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">COCO_PERSON_KEYPOINT_NAMES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;nose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_eye&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_eye&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_ear&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_ear&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_shoulder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_shoulder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_elbow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_elbow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_wrist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_wrist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_hip&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_hip&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_knee&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_knee&#39;</span><span class="p">,</span>
    <span class="s1">&#39;left_ankle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;right_ankle&#39;</span>
<span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<div class="section" id="runtime-characteristics">
<h3>Runtime characteristics<a class="headerlink" href="#runtime-characteristics" title="Permalink to this headline">¶</a></h3>
<p>The implementations of the models for object detection, instance segmentation
and keypoint detection are efficient.</p>
<p>In the following table, we use 8 GPUs to report the results. During training,
we use a batch size of 2 per GPU for all models except SSD which uses 4
and SSDlite which uses 24. During testing a batch size  of 1 is used.</p>
<p>For test time, we report the time for the model evaluation and postprocessing
(including mask pasting in image), but not the time for computing the
precision-recall.</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>train time (s / it)</p></th>
<th class="head"><p>test time (s / it)</p></th>
<th class="head"><p>memory (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Faster R-CNN ResNet-50 FPN</p></td>
<td><p>0.2288</p></td>
<td><p>0.0590</p></td>
<td><p>5.2</p></td>
</tr>
<tr class="row-odd"><td><p>Faster R-CNN MobileNetV3-Large FPN</p></td>
<td><p>0.1020</p></td>
<td><p>0.0415</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-even"><td><p>Faster R-CNN MobileNetV3-Large 320 FPN</p></td>
<td><p>0.0978</p></td>
<td><p>0.0376</p></td>
<td><p>0.6</p></td>
</tr>
<tr class="row-odd"><td><p>FCOS ResNet-50 FPN</p></td>
<td><p>0.1450</p></td>
<td><p>0.0539</p></td>
<td><p>3.3</p></td>
</tr>
<tr class="row-even"><td><p>RetinaNet ResNet-50 FPN</p></td>
<td><p>0.2514</p></td>
<td><p>0.0939</p></td>
<td><p>4.1</p></td>
</tr>
<tr class="row-odd"><td><p>SSD300 VGG16</p></td>
<td><p>0.2093</p></td>
<td><p>0.0744</p></td>
<td><p>1.5</p></td>
</tr>
<tr class="row-even"><td><p>SSDlite320 MobileNetV3-Large</p></td>
<td><p>0.1773</p></td>
<td><p>0.0906</p></td>
<td><p>1.5</p></td>
</tr>
<tr class="row-odd"><td><p>Mask R-CNN ResNet-50 FPN</p></td>
<td><p>0.2728</p></td>
<td><p>0.0903</p></td>
<td><p>5.4</p></td>
</tr>
<tr class="row-even"><td><p>Keypoint R-CNN ResNet-50 FPN</p></td>
<td><p>0.3789</p></td>
<td><p>0.1242</p></td>
<td><p>6.8</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id63">
<h3>Faster R-CNN<a class="headerlink" href="#id63" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn" title="torchvision.models.detection.fasterrcnn_resnet50_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.fasterrcnn_resnet50_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn.html#torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn" title="torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a high resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn.html#torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn" title="torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a low resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone tunned for mobile use-cases.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id64">
<h3>FCOS<a class="headerlink" href="#id64" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.fcos_resnet50_fpn" title="torchvision.models.detection.fcos_resnet50_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.fcos_resnet50_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a FCOS model with a ResNet-50-FPN backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id65">
<h3>RetinaNet<a class="headerlink" href="#id65" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.retinanet_resnet50_fpn.html#torchvision.models.detection.retinanet_resnet50_fpn" title="torchvision.models.detection.retinanet_resnet50_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.retinanet_resnet50_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id66">
<h3>SSD<a class="headerlink" href="#id66" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.ssd300_vgg16.html#torchvision.models.detection.ssd300_vgg16" title="torchvision.models.detection.ssd300_vgg16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.ssd300_vgg16</span></code></a>([…])</p></td>
<td><p>Constructs an SSD model with input size 300x300 and a VGG16 backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id67">
<h3>SSDlite<a class="headerlink" href="#id67" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.ssdlite320_mobilenet_v3_large" title="torchvision.models.detection.ssdlite320_mobilenet_v3_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.ssdlite320_mobilenet_v3_large</span></code></a>([…])</p></td>
<td><p><p>Constructs an SSDlite model with input size 320x320 and a MobileNetV3 Large backbone, as described at <a class="reference external" href="https://arxiv.org/abs/1905.02244">“Searching for MobileNetV3”</a> and <a class="reference external" href="https://arxiv.org/abs/1801.04381">“MobileNetV2: Inverted Residuals and Linear Bottlenecks”</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id70">
<h3>Mask R-CNN<a class="headerlink" href="#id70" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn" title="torchvision.models.detection.maskrcnn_resnet50_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.maskrcnn_resnet50_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="keypoint-r-cnn">
<h3>Keypoint R-CNN<a class="headerlink" href="#keypoint-r-cnn" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.keypointrcnn_resnet50_fpn" title="torchvision.models.detection.keypointrcnn_resnet50_fpn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.detection.keypointrcnn_resnet50_fpn</span></code></a>([…])</p></td>
<td><p>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="video-classification">
<h2>Video classification<a class="headerlink" href="#video-classification" title="Permalink to this headline">¶</a></h2>
<p>We provide models for action recognition pre-trained on Kinetics-400.
They have all been trained with the scripts provided in <code class="docutils literal notranslate"><span class="pre">references/video_classification</span></code>.</p>
<p>All pre-trained models expect input images normalized in the same way,
i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W),
where H and W are expected to be 112, and T is a number of video frames in a clip.
The images have to be loaded in to a range of [0, 1] and then normalized
using <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">[0.43216,</span> <span class="pre">0.394666,</span> <span class="pre">0.37645]</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">[0.22803,</span> <span class="pre">0.22145,</span> <span class="pre">0.216989]</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The normalization parameters are different from the image classification ones, and correspond
to the mean and std from Kinetics-400.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For now, normalization code can be found in <code class="docutils literal notranslate"><span class="pre">references/video_classification/transforms.py</span></code>,
see the <code class="docutils literal notranslate"><span class="pre">Normalize</span></code> function there. Note that it differs from standard normalization for
images because it assumes the video is 4d.</p>
</div>
<p>Kinetics 1-crop accuracies for clip length 16 (16x112x112)</p>
<table class="docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>Clip acc&#64;1</p></th>
<th class="head"><p>Clip acc&#64;5</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ResNet 3D 18</p></td>
<td><p>52.75</p></td>
<td><p>75.45</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet MC 18</p></td>
<td><p>53.90</p></td>
<td><p>76.29</p></td>
</tr>
<tr class="row-even"><td><p>ResNet (2+1)D</p></td>
<td><p>57.50</p></td>
<td><p>78.81</p></td>
</tr>
</tbody>
</table>
<div class="section" id="resnet-3d">
<h3>ResNet 3D<a class="headerlink" href="#resnet-3d" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.video.r3d_18.html#torchvision.models.video.r3d_18" title="torchvision.models.video.r3d_18"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.video.r3d_18</span></code></a>([…])</p></td>
<td><p>Construct 18 layer Resnet3D model as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="resnet-mixed-convolution">
<h3>ResNet Mixed Convolution<a class="headerlink" href="#resnet-mixed-convolution" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.video.mc3_18.html#torchvision.models.video.mc3_18" title="torchvision.models.video.mc3_18"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.video.mc3_18</span></code></a>([…])</p></td>
<td><p>Constructor for 18 layer Mixed Convolution network as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="resnet-2-1-d">
<h3>ResNet (2+1)D<a class="headerlink" href="#resnet-2-1-d" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.video.r2plus1d_18.html#torchvision.models.video.r2plus1d_18" title="torchvision.models.video.r2plus1d_18"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.video.r2plus1d_18</span></code></a>([…])</p></td>
<td><p>Constructor for the 18 layer deep R(2+1)D network as in <a class="reference external" href="https://arxiv.org/abs/1711.11248">https://arxiv.org/abs/1711.11248</a></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="optical-flow">
<h2>Optical flow<a class="headerlink" href="#optical-flow" title="Permalink to this headline">¶</a></h2>
<div class="section" id="raft">
<h3>Raft<a class="headerlink" href="#raft" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.optical_flow.raft_large.html#torchvision.models.optical_flow.raft_large" title="torchvision.models.optical_flow.raft_large"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.optical_flow.raft_large</span></code></a>(*)</p></td>
<td><p>RAFT model from <a class="reference external" href="https://arxiv.org/abs/2003.12039">RAFT: Recurrent All Pairs Field Transforms for Optical Flow</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.optical_flow.raft_small.html#torchvision.models.optical_flow.raft_small" title="torchvision.models.optical_flow.raft_small"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchvision.models.optical_flow.raft_small</span></code></a>(*)</p></td>
<td><p><p>RAFT “small” model from <a class="reference external" href="https://arxiv.org/abs/2003.12039">RAFT: Recurrent All Pairs Field Transforms for Optical Flow</a>.</p>
</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torchvision.models.alexnet.html" class="btn btn-neutral float-right" title="alexnet" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="generated/torchvision.transforms.functional.vflip.html" class="btn btn-neutral" title="vflip" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-present, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Models and pre-trained weights</a><ul>
<li><a class="reference internal" href="#classification">Classification</a><ul>
<li><a class="reference internal" href="#id1">Alexnet</a></li>
<li><a class="reference internal" href="#id2">VGG</a></li>
<li><a class="reference internal" href="#id10">ResNet</a></li>
<li><a class="reference internal" href="#id15">SqueezeNet</a></li>
<li><a class="reference internal" href="#id16">DenseNet</a></li>
<li><a class="reference internal" href="#inception-v3">Inception v3</a></li>
<li><a class="reference internal" href="#id20">GoogLeNet</a></li>
<li><a class="reference internal" href="#shufflenet-v2">ShuffleNet v2</a></li>
<li><a class="reference internal" href="#mobilenet-v2">MobileNet v2</a></li>
<li><a class="reference internal" href="#mobilenet-v3">MobileNet v3</a></li>
<li><a class="reference internal" href="#id25">ResNext</a></li>
<li><a class="reference internal" href="#wide-resnet">Wide ResNet</a></li>
<li><a class="reference internal" href="#id28">MNASNet</a></li>
<li><a class="reference internal" href="#id32">EfficientNet</a></li>
<li><a class="reference internal" href="#id40">RegNet</a></li>
<li><a class="reference internal" href="#id55">VisionTransformer</a></li>
<li><a class="reference internal" href="#id59">ConvNeXt</a></li>
<li><a class="reference internal" href="#quantized-models">Quantized Models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#semantic-segmentation">Semantic Segmentation</a><ul>
<li><a class="reference internal" href="#fully-convolutional-networks">Fully Convolutional Networks</a></li>
<li><a class="reference internal" href="#deeplabv3">DeepLabV3</a></li>
<li><a class="reference internal" href="#lr-aspp">LR-ASPP</a></li>
</ul>
</li>
<li><a class="reference internal" href="#object-detection-instance-segmentation-and-person-keypoint-detection">Object Detection, Instance Segmentation and Person Keypoint Detection</a><ul>
<li><a class="reference internal" href="#runtime-characteristics">Runtime characteristics</a></li>
<li><a class="reference internal" href="#id63">Faster R-CNN</a></li>
<li><a class="reference internal" href="#id64">FCOS</a></li>
<li><a class="reference internal" href="#id65">RetinaNet</a></li>
<li><a class="reference internal" href="#id66">SSD</a></li>
<li><a class="reference internal" href="#id67">SSDlite</a></li>
<li><a class="reference internal" href="#id70">Mask R-CNN</a></li>
<li><a class="reference internal" href="#keypoint-r-cnn">Keypoint R-CNN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#video-classification">Video classification</a><ul>
<li><a class="reference internal" href="#resnet-3d">ResNet 3D</a></li>
<li><a class="reference internal" href="#resnet-mixed-convolution">ResNet Mixed Convolution</a></li>
<li><a class="reference internal" href="#resnet-2-1-d">ResNet (2+1)D</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optical-flow">Optical flow</a><ul>
<li><a class="reference internal" href="#raft">Raft</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>